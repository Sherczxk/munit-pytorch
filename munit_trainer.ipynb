{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from network import Discriminator,Generator\n",
    "from utils import weights_init, recon_loss, gen_loss, dis_loss\n",
    "\n",
    "class MUNIT(nn.Module):\n",
    "    def __init__(self,opt):\n",
    "        super(MUNIT,self).__init__()\n",
    "        \n",
    "        # generators and discriminators\n",
    "        self.gen_a = Generator(opt.ngf,opt.style_dim,opt.mlp_dim)\n",
    "        self.gen_b = Generator(opt.ngf,opt.style_dim,opt.mlp_dim)\n",
    "        self.dis_a = Discriminator(opt.ndf)\n",
    "        self.dis_b = Discriminator(opt.ndf)\n",
    "        #random style code\n",
    "        self.s_a = torch.randn(opt.display_size,opt.style_dim,1,1,requires_grad=True).cuda()\n",
    "        self.s_b = torch.randn(opt.display_size,opt.style_dim,1,1,requires_grad=True).cuda()\n",
    "        \n",
    "        #optimizers\n",
    "        dis_params = list(self.dis_a.parameters())+list(self.dis_b.parameters())\n",
    "        gen_params = list(self.gen_a.parameters())+list(self.gen_b.parameters())\n",
    "        self.dis_opt = torch.optim.Adam(dis_params,lr=opt.lr,beta=opt.beta1,weight_delay=opt.weight_delay)\n",
    "        self.gen_opt = torch.optim.Adam(gen_params,lr=opt.lr,beta=opt.beta1,weight_delay=opt.weight_delay)\n",
    "        \n",
    "        # nerwork weight initialization\n",
    "        self.apply(weights_init('kaiming'))\n",
    "        self.dis_a.apply(weights_init('gaussian'))\n",
    "        self.dis_b.apply(weights_init('gaussian'))\n",
    "        \n",
    "    def forward(self,x_a,x_b):\n",
    "        c_a,s_a_prime = self.gen_a.encode(x_a) \n",
    "        c_b,s_b_prime = self.gen_b.encode(x_b)\n",
    "        x_a2b = self.gen_b.decode(c_a,self.s_b)\n",
    "        x_b2a = self.gen_a.decode(c_b,self.s_a)\n",
    "        return x_a2b,x_b2a\n",
    "    \n",
    "    def backward_G(self,x_a,x_b):\n",
    "    \n",
    "        #encoding and decoding\n",
    "        s_a = torch.randn(x_a.size(0),opt.style_dim,1,1,requires_grad=True).cuda()\n",
    "        s_b = torch.randn(x_b.size(0),opt.style_dim,1,1,requires_grad=True).cuda()\n",
    "        c_a,s_a_prime = self.gen_a.encode(x_a) \n",
    "        c_b,s_b_prime = self.gen_b.encode(x_b)\n",
    "        x_a_rec = self.gen_a.decode(c_a,s_a_prime)\n",
    "        x_b_rec = self.gen_b.decode(c_b,s_b_prime)\n",
    "        x_a2b = self.gen_b.decode(c_a,s_b)\n",
    "        x_b2a = self.gen_a.decode(c_b,s_a)\n",
    "        c_a_rec,s_b_rec = self.gen_b.encode(x_a2b)\n",
    "        c_b_rec,s_a_rec = self.gen_a.encode(x_b2a)\n",
    "        x_a_fake = self.dis_a(x_b2a)\n",
    "        x_b_fake = self.dis_b(x_a2b)\n",
    "            \n",
    "        #loss function\n",
    "        self.loss_xa = recon_loss(x_a_rec-x_a)\n",
    "        self.loss_xb = recon_loss(x_b_rec-x_b)\n",
    "        self.loss_ca = recon_loss(c_a_rec-c_a)\n",
    "        self.loss_cb = recon_loss(c_b_rec-c_b)\n",
    "        self.loss_sa = recon_loss(s_a_rec-s_a)\n",
    "        self.loss_sb = recon_loss(s_b_rec-s_b)\n",
    "        if opt.x_cyc_w>0:\n",
    "            x_a2b2a = self.gen_b.decode(c_a_rec,s_a_prime)\n",
    "            x_b2a2b = self.gen_a.decode(c_b_rec,s_b_prime)\n",
    "            self.loss_cyc_a2b = recon_loss(x_a2b2a,x_a)\n",
    "            self.loss_cyc_b2a = recon_loss(x_b2a2b,x_b)\n",
    "        else:\n",
    "            self.loss_cyc_a2b = 0\n",
    "            self.loss_cyc_b2a = 0\n",
    "        self.loss_gen_a = gen_loss(x_a_fake,opt.gan_type)\n",
    "        self.loss_gen_b = gen_loss(x_b_fake,opt.gan_type)\n",
    "        \n",
    "        self.gen_total_loss = opt.gan_w*(self.loss_gen_a+self.loss_gen_b)+\\\n",
    "                              opt.x_w*(self.loss_xa+self.loss_xb)+\\\n",
    "                              opt.c_w*(self.loss_ca+self.loss_cb)+\\\n",
    "                              opt.s_w*(self.loss_sa+self.loss_sb)+\\\n",
    "                              opt.x_cyc_w*(self.loss_cyc_a2b+self.loss_cyc_b2a)\n",
    "        self.gen_total_loss.backward()\n",
    "        return self.gen_total_loss\n",
    "        \n",
    "    def barkward_D(self,x_a,x_b):\n",
    "        s_a = torch.randn(x_a.size(0),opt.style_dim,1,1,requires_grad=True).cuda()\n",
    "        s_b = torch.randn(x_b.size(0),opt.style_dim,1,1,requires_grad=True).cuda()\n",
    "        c_a,_ = self.gen_a.encode(x_a.detach()) \n",
    "        c_b,_ = self.gen_b.encode(x_b.detach())\n",
    "        x_a2b = self.gen_b.decode(c_a,s_b)\n",
    "        x_b2a = self.gen_a.decode(c_b,s_a)\n",
    "        x_a_real = self.dis_a(x_a.detach())\n",
    "        x_a_fake = self.dis_a(x_b2a)\n",
    "        x_b_real = self.dis_b(x_b.detach())\n",
    "        x_b_fake = self.dis_b(x_a2b)\n",
    "        \n",
    "        self.loss_dis_a = dis_loss(x_a.detach(),x_b2a,opt.gan_type)\n",
    "        self.loss_dis_b = dis_loss(x_b.detach(),x_a2b,opt.gan_type)\n",
    "        self.dis_total_loss = opt.gan_w*(self.loss_dis_a+self.loss.dis_b)\n",
    "        \n",
    "        self.dis_total_loss.backward()\n",
    "        return self.dis_total_loss\n",
    "        \n",
    "    def optimize_parameters(self,x_a,x_b):\n",
    "        self.gen_opt.zero_grad()\n",
    "        self.dis_opt.zero_grad()\n",
    "        self.backward_G(x_a,x_b)\n",
    "        self.barkward_D(x_a,x_b)\n",
    "        self.gen_opt.step()\n",
    "        self.dis_opt.step()\n",
    "        \n",
    "    def sample(self,x_a,x_b,num_style):\n",
    "        x_a2b = []\n",
    "        x_b2a = []\n",
    "        for i in range(x_a.size(0)):\n",
    "            s_a = torch.randn(num_style,opt.style_dim,1,1).cuda()\n",
    "            s_b = torch.randn(num_style,opt.style_dim,1,1).cuda()\n",
    "            c_a_i,_ = self.gen_a.encode(x_a[i].unsqueeze(0)) \n",
    "            c_b_i,_ = self.gen_b.encode(x_b[i].unsqueeze(0))\n",
    "            x_i_a2b = []\n",
    "            x_i_b2a = []\n",
    "            for j in range(num_style):\n",
    "                #[1,opt.style_dim,1,1]\n",
    "                s_a_j = s_a[j].unsqueeze(0) \n",
    "                s_b_j = s_b[j].unsqueeze(0)\n",
    "                x_i_a2b.append(self.gen_b.decode(c_a_i,s_b_j))\n",
    "                x_i_b2a.append(self.gen_a.decode(c_b_i,s_a_j))\n",
    "            #[num_style,c,h,w]\n",
    "            x_i_a2b = torch.cat(x_i_a2b,dim=0) \n",
    "            x_i_b2a = torch.cat(x_i_b2a,dim=0)\n",
    "            x_a2b.append(x_i_a2b)\n",
    "            x_b2a.append(x_i_b2a)\n",
    "        #[batch_size,num_style,c,h,w]\n",
    "        x_a2b = torch.stack(x_a2b)\n",
    "        x_b2a = torch.stack(x_b2a)\n",
    "        #[batch_size*num_style,c,h,w]\n",
    "        x_a2b = x_a2b.view(-1,x_a2b.size()[2:])\n",
    "        x_b2a = x_b2a.view(-1,x_b2a.size()[2:])\n",
    "        return x_a2b,x_b2a\n",
    "        \n",
    "        \n",
    "    def test(self,input,direction,style):\n",
    "        output = []\n",
    "        if direction == 'a2b':\n",
    "            encoder = self.gen_a.encode()\n",
    "            decoder = self.gen_b.decode()\n",
    "        else:\n",
    "            encoder = self.gen_b.encode()\n",
    "            decoder = self.gen_a.decode()\n",
    "        content,_ = encode(input.unsqueeze(0))\n",
    "        for i in range(style.size()):\n",
    "            output.append(decoder(content,style[i].unsqueeze(0)))\n",
    "        output = torch.cat(output,dim=0)\n",
    "        return output\n",
    "            \n",
    "        \n",
    "    def save(self,save_dir,iterations):\n",
    "        save_gen = os.path.join(save_dir,'gen_%8d.pt'%(iteration+1))\n",
    "        save_dis = os.path.join(save_dir,'dis_%8d.pt'%(iteration+1))\n",
    "        save_opt = os.path.join(save_dir,'optimizer.pt')\n",
    "        torch.save({'a':self.gen_a.state_dict(),'b':self.gen_b.state_dict()},save_gen)\n",
    "        torch.save({'a':self.dis_a.state_dict(),'b':self.dis_b.state_dict()},save_dis)\n",
    "        torch.save({'gen':self.gen_opt.state_dict(),'dis':self.dis_opt.state_dict()},save_opt)\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
