{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "##############################################################\n",
    "#  Discriminator\n",
    "##############################################################\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,ndf):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.ndf = ndf\n",
    "        self.norm = 'none'\n",
    "        self.activ = 'lrelu'\n",
    "        self.pad = 'reflect'\n",
    "        self.num_scales = 3\n",
    "        self.num_layer = 4\n",
    "        self.downsample = nn.AvgPool2d(3,stride=2,padding=[1,1],count_include_pad=False)\n",
    "        self.model = nn.ModuleList()\n",
    "        for i in range(self.num_scales):\n",
    "            self.model.append(self.scale_dis())\n",
    "    \n",
    "    def scale_dis(self):\n",
    "        ndf = self.ndf\n",
    "        dis_model = []\n",
    "        dis_model += [Pad( self.pad,1),nn.Conv2d(3,ndf,4,2),Norm( norm_type='none',dim=ndf),Activ( self.activ)]\n",
    "        for i in range(self.num_layer-1):\n",
    "            dis_model += [Pad( self.pad_type,1),nn.Conv2d(ndf,ndf*2,4,2),Norm( self.norm,ndf*2),Activ( self.activ)]\n",
    "            ndf *= 2\n",
    "        dis_model += [nn.Conv2d(ndf,1,1,1,0)]\n",
    "        dis_model = nn.Sequential(*dis_model)\n",
    "        return dis_model\n",
    "        \n",
    "    def forward(self,x):\n",
    "        outputs = []\n",
    "        #discriminator coarse2fine\n",
    "        for i in range(self.num_scales):\n",
    "            outputs.append(self.scale_dis(x))\n",
    "            x = self.downsample(x)\n",
    "        return outputs\n",
    "###############################################################\n",
    "#  Generator\n",
    "###############################################################\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self,ngf,style_dim,mlp_dim):\n",
    "        super(Generator,self).__init__()\n",
    "        self.ngf = ngf\n",
    "        self.style_dim = style_dim\n",
    "        self.mlp_dim = mlp_dim\n",
    "    \n",
    "        self.Es = Style_Encoder(ngf,style_dim)\n",
    "        self.Ec = Content_Encoder(ngf)\n",
    "    \n",
    "    def encode(self,x):\n",
    "        content_code,_ = self.Ec(x)\n",
    "        style_code = self.Es(x)\n",
    "        return content_code,style_code\n",
    "        \n",
    "    def decode(self,content,style):\n",
    "        content_dim = content.size(1)\n",
    "        images = Decoder(self.style_dim,content_dim,self.mlp_dim,content,style)\n",
    "        return images\n",
    "\n",
    "    def forward(self,x):\n",
    "        content,style = self.encode(x)\n",
    "        x_rec = self.decode(content,style)\n",
    "        return x_rec\n",
    "        \n",
    "#------------------------- Style Encoder -------------------------\n",
    "class Style_Encoder(nn.Module):\n",
    "    def __init__(self,ngf,style_dim):\n",
    "        super(Style_Encoder,self).__init__()\n",
    "        self.num_downsample_1 = 2\n",
    "        self.num_downsample_2 = 2\n",
    "        self.ngf = ngf\n",
    "        self.style_dim = style_dim\n",
    "        self.norm = 'none'\n",
    "        self,activ = 'relu'\n",
    "        self.pad = 'reflect'\n",
    "        \n",
    "        #  downsample->global_pooling->FC_layer\n",
    "        self.model = []\n",
    "        self.model += [Pad( self.pad,3),nn.Conv2d(3,ngf,7,1),Norm( self.norm,self.ngf),Activ( self.activ)]\n",
    "        self.model += [self.downsampling_blocks()]\n",
    "        self.model += [self.global_pooling()]\n",
    "        self.model += [self.FC_layer()]\n",
    "        self.model = nn.Sequential(*self.model)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def downsampling_blocks(self):\n",
    "        ngf = self.ngf\n",
    "        m = []\n",
    "        for i in range(self.num_downsample_1):\n",
    "            m += [Pad( self.pad,1),nn.Conv2d(ngf,ngf*2,4,2),Norm( self.norm,ngf*2),Activ( self.activ)]\n",
    "            ngf *= 2\n",
    "        for i in range(self.num_downsample_2):\n",
    "            m += [Pad( self.pad,1),nn.Conv2d(ngf,ngf,4,2),Norm( self.norm,ngf),Activ( self.activ)]\n",
    "        m = nn.Sequential(*m)\n",
    "        return m\n",
    "    \n",
    "    def global_pooling(self):\n",
    "        return nn.AdaptiveAvgPooling2d(1)\n",
    "        \n",
    "    def FC_layer(self):\n",
    "        m = nn.Sequential([Pad( self.pad,0),nn.Conv2d(self.ngf,self.style_dim,1,1)])\n",
    "        return m\n",
    "    \n",
    "#------------------------- Content Encoder -----------------------\n",
    "class Content_Encoder(nn.Module):\n",
    "    def __init__(self,ngf):\n",
    "        super(Content_Encoder,self).__init__()\n",
    "        self.num_downsample = 2\n",
    "        self.num_res = 4\n",
    "        self.ngf = ngf\n",
    "        self.norm = 'in'\n",
    "        self.activ = 'relu'\n",
    "        self.pad = 'reflect'\n",
    "        \n",
    "        #  downsample->residual blocks\n",
    "        self.model = []\n",
    "        self.model += [Pad(self.pad,3),nn.Conv2d(3,ngf,7,1),Norm(self.norm,self.ngf),Activ(self.activ)]\n",
    "        self.model += [self.downsampling_blocks()]\n",
    "        self.model += [self.residual_blocks()]\n",
    "        self.model = nn.Sequential(*self.model)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "        \n",
    "    def downsampling_blocks(self):\n",
    "        ngf = self.ngf\n",
    "        m = []\n",
    "        for i in range(self.num_downsample):\n",
    "            m += [Pad( self.pad,1),nn.Conv2d(ngf,ngf*2,4,2),Norm( self.norm,ngf*2),Activ( self.activ)]\n",
    "            ngf *= 2\n",
    "        m = nn.Sequential(*m)\n",
    "        return m\n",
    "        \n",
    "    def residual_blocks(self):\n",
    "        ngf = self.ngf\n",
    "        m = []\n",
    "        for i in range(self.num_res):\n",
    "            m += [residual_block(ngf,self.norm,self.activ,self.pad_type)]\n",
    "        m = nn.Sequential(*m)\n",
    "        return m\n",
    "        \n",
    "class residual_block(nn.Module):\n",
    "    def __init__(self,dim,norm,activ,pad_type):\n",
    "        super(residual_block,self).__init__()\n",
    "        \n",
    "        self.block = []\n",
    "        self.block += [Pad(pad_type,1),nn.Conv2d(dim,dim,3,1),Norm(norm,dim),Activ(activ)]\n",
    "        self.block += [Pad(pad_type,1),nn.Conv2d(dim,dim,3,1),Norm(norm),Activ(activ_type = 'none')]\n",
    "        self.block = nn.Sequential(*self.block)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        input = x\n",
    "        x = self.block(x)\n",
    "        x += input\n",
    "        return x\n",
    "    \n",
    "#------------------------- Decoder -------------------------------\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,style_dim,content_dim,mlp_dim):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.num_upsamle = 2\n",
    "        self.num_res = 4\n",
    "        self.content_dim = content_dim \n",
    "        self.activ = 'relu'\n",
    "        self.pad = 'zero'\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "        self.num_adain_params = 2*2*content_dim*num_res\n",
    "        self.mlp = MLP(style_dim,num_adain_params,mlp_dim)\n",
    "    \n",
    "        # AdIN_residual_blocks->upsampling\n",
    "        \n",
    "        self.AdainRes = self.AdIN_residual_block(self.content_dim,self.activ,self.pad_type)\n",
    "        self.upsampling = self.upsampling_blocks()\n",
    "        \n",
    "    def forward(self,content,style):\n",
    "        input_dim = self.content_dim\n",
    "        adain_params = self.mlp(style)\n",
    "        #AdIN_residual_blocks\n",
    "        for i in range(self.num_res):\n",
    "            adain_params_i = adain_params[:,input_dim*i:4*input_dim*(i+1)]\n",
    "            content = self.AdainRes(adain_params_i,content)\n",
    "        #upsampling\n",
    "        content = self.upsampling(content)\n",
    "        return content\n",
    "        \n",
    "    def upsampling_blocks(self):\n",
    "        input_dim = self.content_dim\n",
    "        m = []\n",
    "        for i in range(self.num_upsamle):\n",
    "            m += [self.upsample,Pad(self.pad,2),nn.Conv2d(input_dim,input_dim//2,5,1),Norm(norm_type='ln',dim=input_dim//2),Activ(self.activ)]\n",
    "            input_dim //=2\n",
    "        m += [Pad(pad_type='reflect',padding=3),nn.Conv2d(self.content_dim,3,7,1),Norm(norm_type='none',dim=3),Activ(activ_type='tanh')]\n",
    "        m = nn.Sequential(*m)\n",
    "        return m\n",
    "        \n",
    "class AdIN_residual_block(nn.Module):\n",
    "    def __init__(self,dim,activ,pad_type):\n",
    "        super(AdIN_residual_blocks,self).__init__()\n",
    "        self.dim = dim\n",
    "        self.norm_1 = AdainNorm2d(dim)\n",
    "        self.norm_2 = AdainNorm2d(dim)\n",
    "        \n",
    "        self.model = []\n",
    "        self.model += [Pad(pad_type,1),nn.Conv2d(dim,dim,3,1),self.norm_1,Activ(activ)]\n",
    "        self.model += [Pad(pad_type,1),nn.Conv2d(dim,dim,3,1),self.norm_2,Activ(activ_type = 'none')]\n",
    "        self.model = nn.Sequential(*self.model)\n",
    "        \n",
    "    def forward(self,adain_params,x):\n",
    "        dim = self.dim\n",
    "        self.norm_1.beta = adain_params[:,:dim]\n",
    "        self.norm_1.gamma = adain_params[:,dim:2*dim]\n",
    "        self.norm_2.beta = adain_params[:,2*dim:3*dim]\n",
    "        self.norm_2.gamma = adain_params[:,3*dim:4*dim]\n",
    "        input = x\n",
    "        x = self.model(x)\n",
    "        x += input\n",
    "        return x\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    MLP is to calculate the parameters (gamma,beta) of AdainNorm2d\n",
    "    '''\n",
    "    def __init__(self,input_dim,output_dim,mlp_dim):\n",
    "        super(MLP,self).__init__()\n",
    "        self.num_layer = 3\n",
    "        self.norm = 'none'\n",
    "        self.activ = 'relu'\n",
    "        \n",
    "        self.model = []\n",
    "        self.model += [nn.Linear(input_dim,mlp_dim,bias=True),Norm(self.norm,mlp_dim),Activ(self.activ)]\n",
    "        for i in range(self.num_layer-2):\n",
    "            self.model += [nn.Linear(mlp_dim,mlp_dim,bias=True),Norm(self.norm,mlp_dim),Activ(self.activ)]\n",
    "        self.model += [nn.Linear(mlp_dim,output_dim,bias=True),Norm(norm_type='none',dim=output_dim),Activ(activ_type='none')]\n",
    "        self.model = nn.Sequential(*self.model)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x.view(x.size(0),-1)\n",
    "        return self.model(x)\n",
    "\n",
    "class AdainNorm2d(nn.Module):\n",
    "    def __init__(self,number_features,eps=1e-5,momentum=0.1):\n",
    "        super(AdainNorm2d,self).__init__()\n",
    "        self.number_features = number_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        # gamma,beta will be  dynamically assigned by MLP\n",
    "        self.gamma = None\n",
    "        self.beta = None\n",
    "    \n",
    "    def forward(self,x):\n",
    "        b,c,h,w = x.size()\n",
    "        mean = torch.zeros(self.number_features*b)\n",
    "        var = torch.ones(self.number_features*b)\n",
    "        x_reshaped = x.contiguous().view(1,b*c,h,w)\n",
    "        x_new = F.batch_norm(x_reshaped,mean,var,self.gamma,self.beta,True,self.momentum,self.eps)\n",
    "        return x_new.view(b,c,h,w)\n",
    "    \n",
    "    \n",
    "###########################  function ###############################\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity,self).__init__()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return x\n",
    "    \n",
    "def Norm(norm_type='none',dim=64):\n",
    "    \n",
    "    if norm_type == 'in':\n",
    "        norm_type = nn.InstanceNorm2d(dim)\n",
    "    elif norm_type == 'bn':\n",
    "        norm_type = nn.BatchNorm2d(dim)\n",
    "    elif norm_type == 'ln':\n",
    "        norm_type = nn.LayerNorm(dim)\n",
    "    elif norm_type == 'none':\n",
    "        norm_type = Identity()\n",
    "    else:\n",
    "        raise NotImplementedError('normalization method [%s] is not implemented' % norm_type)\n",
    "    return norm_type\n",
    "        \n",
    "def Activ(activ_type='none'):\n",
    "    if activ_type == 'relu':\n",
    "        activ_type = nn.ReLU(inplace=True)\n",
    "    elif activ_type == 'lrelu':\n",
    "        activ_type = nn.LeakyReLU(0.2,inplace=True)\n",
    "    elif activ_type == 'prelu':\n",
    "        activ_type = nn.PReLU()\n",
    "    elif activ_type == 'tanh':\n",
    "        activ_type = nn.Tanh()\n",
    "    elif activ_type == 'none':\n",
    "        activ_type = Identity()\n",
    "    else:\n",
    "        raise NotImplementedError('activation method [%s] is not implemented' % activ_type)\n",
    "    return activ_type\n",
    "    \n",
    "def Pad(pad_type='zero',padding=0):\n",
    "    if pad_type == 'reflect':\n",
    "        pad_type = nn.ReflectionPad2d(padding)\n",
    "    elif pad_type == 'replicate':\n",
    "        pad_type = nn.RepicationPad2d(padding)\n",
    "    elif pad_type == 'zero':\n",
    "        pad_type = nn.ZeroPad2d(padding)\n",
    "    else:\n",
    "        raise NotImplementedError('padding method [%s] is not implemented' % pad_type)\n",
    "    return pad_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
